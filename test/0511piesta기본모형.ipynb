{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d4ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, csv_file: str):\n",
    "        self.csv_file = csv_file\n",
    "        self.prices = self._read_csv()\n",
    "\n",
    "    def _read_csv(self):\n",
    "        prices = pd.read_csv(self.csv_file, index_col=0, parse_dates=True)\n",
    "        return prices\n",
    "\n",
    "    def load_data(self, start_date: str, end_date: str):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        return self.prices.loc[start_date:end_date]\n",
    "\n",
    "    def get_prices(self, assets: list = None):\n",
    "        if assets is not None:\n",
    "            return self.prices[assets]\n",
    "        return self.prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5326273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any, Tuple, Callable\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 임시\n",
    "def historical_return(assets, window=20):\n",
    "    pass\n",
    "\n",
    "def historical_variance(assets, window=20):\n",
    "    pass\n",
    "\n",
    "def semi_covariance(assets):\n",
    "    pass\n",
    "\n",
    "def historical_covariance(assets, window=20):\n",
    "    pass\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name: str, **params: Any):\n",
    "        self.name = name\n",
    "        self.children: List[Node] = []\n",
    "        self.params = params\n",
    "\n",
    "    def add_child(self, child_node) -> None:\n",
    "        self.children.append(child_node)\n",
    "        \n",
    "class Tree:\n",
    "    def __init__(self, root_name: str):\n",
    "        self.root = Node(root_name)\n",
    "        #self.assumption = AssetAssumption\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'Tree({self.root.name})'\n",
    "\n",
    "    def insert(self, parent_name: str, child_name: str, **params: Any) -> bool:\n",
    "        parent_node = self._find_node(self.root, parent_name)\n",
    "        if parent_node:\n",
    "            child_node = Node(child_name, **params)\n",
    "            parent_node.add_child(child_node)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def draw(self) -> None:\n",
    "        lines = self._build_tree_string(self.root, '')\n",
    "        print('\\n'.join(lines))\n",
    "\n",
    "    def _find_node(self, node: Node, target_name: str) -> Optional[Node]:\n",
    "        if node.name == target_name:\n",
    "            return node\n",
    "        for child in node.children:\n",
    "            found_node = self._find_node(child, target_name)\n",
    "            if found_node:\n",
    "                return found_node\n",
    "        return None\n",
    "\n",
    "    def _build_tree_string(self, node: Node, prefix: str, is_tail: bool = True):\n",
    "        lines = []\n",
    "        line = f\"{prefix}{'`-- ' if is_tail else '|-- '}{node.name}\"\n",
    "        lines.append(line)\n",
    "        prefix += '    ' if is_tail else '|   '\n",
    "        child_count = len(node.children)\n",
    "        for i, child in enumerate(node.children):\n",
    "            is_last_child = i == child_count - 1\n",
    "            lines.extend(self._build_tree_string(child, prefix, is_last_child))\n",
    "        return lines  # Add the missing return statement\n",
    "    \n",
    "    def get_all_nodes(self) -> List[Node]:\n",
    "        return self._collect_nodes(self.root, [])\n",
    "\n",
    "    def _collect_nodes(self, node: Node, node_list: List[Node]) -> List[Node]:\n",
    "        for child in node.children:\n",
    "            node_list.append(child)\n",
    "            self._collect_nodes(child, node_list)\n",
    "        return node_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956304a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nassumption = AssetAssumption(returns=(historical_return, {'window': 20}), \\n                             covariance=(historical_covariance, {'window': 60}))\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Callable, Any\n",
    "\n",
    "class AssetAssumption:\n",
    "    def __init__(self, **params: Any):\n",
    "        self.param_dict = params\n",
    "        self.param_key = [key for key in params.keys()]\n",
    "\n",
    "        self.param = {}\n",
    "        self.max_window = 0\n",
    "        \n",
    "    def get_data(self, loader: Loader, start_date, end_date):\n",
    "        # Extract the maximum window from the input parameters\n",
    "        for key in self.param_key:\n",
    "            _, kwargs = self.param_dict[key]\n",
    "            if 'window' in kwargs and kwargs['window'] > self.max_window:\n",
    "                self.max_window = kwargs['window']\n",
    "\n",
    "        start_date = self.base_date(loader, start_date, self.max_window)\n",
    "\n",
    "        price_data = loader.load_data(start_date, end_date)\n",
    "        self.compute_parameters(price_data)\n",
    "        \n",
    "        return self.param\n",
    "        \n",
    "    def compute_parameters(self, price_data):\n",
    "        daily_rtn = price_data.pct_change().dropna()\n",
    "        for keys in self.param_key:\n",
    "            func, kwargs = self.param_dict[keys]\n",
    "            self.param[keys] = func(daily_rtn, **kwargs)\n",
    "        \n",
    "    def base_date(self, loader: Loader, start_date, window):\n",
    "        start_date = start_date - timedelta(days=window+30)\n",
    "        return start_date\n",
    "    \n",
    "    \n",
    "def historical_return(daily_rtn_data:Optional[pd.DataFrame]=None, window=20):\n",
    "    if isinstance(daily_rtn_data, pd.DataFrame):\n",
    "        historical_return = daily_rtn_data.rolling(window=window).mean()*100*252\n",
    "        return historical_return.dropna()\n",
    "    \n",
    "\n",
    "def historical_variance(daily_rtn_data:Optional[pd.DataFrame]=None, window=20):\n",
    "    if isinstance(daily_rtn_data, pd.DataFrame):\n",
    "        historical_var = daily_rtn_data.rolling(window=window).std()*np.sqrt(252)\n",
    "        return historical_var.dropna()\n",
    "\n",
    "def historical_covariance(daily_rtn_data:Optional[pd.DataFrame]=None, window=20):\n",
    "    if isinstance(daily_rtn_data, pd.DataFrame):\n",
    "        historical_cov = daily_rtn_data.rolling(window=window).cov()\n",
    "        return historical_cov.dropna()\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "assumption = AssetAssumption(returns=(historical_return, {'window': 20}), \n",
    "                             covariance=(historical_covariance, {'window': 60}))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762c8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "\n",
    "class InstantiationError(Exception):\n",
    "    pass\n",
    "\n",
    "class OptimizationError(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ace825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_matrix(nodes:List[Node]):\n",
    "    cov_dict = {}\n",
    "\n",
    "    node_names = [node.name for node in nodes]  # Get the names of the nodes in the list\n",
    "    \n",
    "    for node in nodes:\n",
    "        if 'covariance' in [i for i in node.params.keys()]:\n",
    "            cov_series = node.params['covariance']\n",
    "        else:\n",
    "            print('공분산 정보가 없습니다')\n",
    "            return None\n",
    "        \n",
    "        # Select only the covariances for the nodes in the list\n",
    "        selected_covariances = cov_series[cov_series.index.isin(node_names)]\n",
    "        cov_dict[node.name] = selected_covariances\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    cov_matrix = pd.DataFrame(cov_dict)\n",
    "\n",
    "    # Since the covariance of A with B is the same as B with A, we can fill the NaN values\n",
    "    # by transposing the matrix, filling NaN values, and then transposing again\n",
    "    cov_matrix = cov_matrix.T.fillna(cov_matrix).T\n",
    "    \n",
    "    print(cov_matrix.to_numpy())\n",
    "    \n",
    "    return cov_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3321bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cvxpy as cp\n",
    "import scipy.optimize as sco\n",
    "\n",
    "class BaseOptimizer:\n",
    "    \n",
    "    def __init__(self, n_assets, tickers=None):\n",
    "        self.n_assets = n_assets\n",
    "        if tickers is None:\n",
    "            self.tickers = list(range(n_assets))\n",
    "        else:\n",
    "            self.tickers = tickers\n",
    "        self._risk_free_rate = None\n",
    "        # Outputs\n",
    "        self.weights = None\n",
    "\n",
    "    def _make_output_weights(self, weights=None):\n",
    "        if weights is None:\n",
    "            weights = self.weights\n",
    "\n",
    "        return collections.OrderedDict(zip(self.tickers, weights))\n",
    "\n",
    "    def set_weights(self, input_weights):\n",
    "        self.weights = np.array([input_weights[ticker] for ticker in self.tickers])\n",
    "\n",
    "    def clean_weights(self, cutoff=1e-4, rounding=5):\n",
    "        if self.weights is None:\n",
    "            raise AttributeError(\"Weights not yet computed\")\n",
    "        clean_weights = self.weights.copy()\n",
    "        clean_weights[np.abs(clean_weights) < cutoff] = 0\n",
    "        if rounding is not None:\n",
    "            if not isinstance(rounding, int) or rounding < 1:\n",
    "                raise ValueError(\"rounding must be a positive integer\")\n",
    "            clean_weights = np.round(clean_weights, rounding)\n",
    "\n",
    "        return self._make_output_weights(clean_weights)\n",
    "\n",
    "\n",
    "class BaseConvexOptimizer(BaseOptimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_assets,\n",
    "        tickers=None,\n",
    "        weight_bounds=(0, 1),\n",
    "        solver=None,\n",
    "        verbose=False,\n",
    "        solver_options=None,\n",
    "    ):\n",
    "\n",
    "        super().__init__(n_assets, tickers)\n",
    "\n",
    "        # Optimization variables\n",
    "        self._w = cp.Variable(n_assets)\n",
    "        self._objective = None\n",
    "        self._additional_objectives = []\n",
    "        self._constraints = []\n",
    "        self._lower_bounds = None\n",
    "        self._upper_bounds = None\n",
    "        self._opt = None\n",
    "        self._solver = solver\n",
    "        self._verbose = verbose\n",
    "        self._solver_options = solver_options if solver_options else {}\n",
    "        self._map_bounds_to_constraints(weight_bounds)\n",
    "\n",
    "    def deepcopy(self):\n",
    "\n",
    "        self_copy = copy.copy(self)\n",
    "        self_copy._additional_objectives = [\n",
    "            copy.copy(obj) for obj in self_copy._additional_objectives\n",
    "        ]\n",
    "        self_copy._constraints = [copy.copy(con) for con in self_copy._constraints]\n",
    "        return self_copy\n",
    "\n",
    "    def _map_bounds_to_constraints(self, test_bounds):\n",
    "\n",
    "        # If it is a collection with the right length, assume they are all bounds.\n",
    "        if len(test_bounds) == self.n_assets and not isinstance(\n",
    "            test_bounds[0], (float, int)\n",
    "        ):\n",
    "            bounds = np.array(test_bounds, dtype=float)\n",
    "            self._lower_bounds = np.nan_to_num(bounds[:, 0], nan=-np.inf)\n",
    "            self._upper_bounds = np.nan_to_num(bounds[:, 1], nan=np.inf)\n",
    "        else:\n",
    "            # Otherwise this must be a pair.\n",
    "            if len(test_bounds) != 2 or not isinstance(test_bounds, (tuple, list)):\n",
    "                raise TypeError(\n",
    "                    \"test_bounds must be a pair (lower bound, upper bound) OR a collection of bounds for each asset\"\n",
    "                )\n",
    "            lower, upper = test_bounds\n",
    "\n",
    "            # Replace None values with the appropriate +/- 1\n",
    "            if np.isscalar(lower) or lower is None:\n",
    "                lower = -1 if lower is None else lower\n",
    "                self._lower_bounds = np.array([lower] * self.n_assets)\n",
    "                upper = 1 if upper is None else upper\n",
    "                self._upper_bounds = np.array([upper] * self.n_assets)\n",
    "            else:\n",
    "                self._lower_bounds = np.nan_to_num(lower, nan=-1)\n",
    "                self._upper_bounds = np.nan_to_num(upper, nan=1)\n",
    "\n",
    "        self.add_constraint(lambda w: w >= self._lower_bounds)\n",
    "        self.add_constraint(lambda w: w <= self._upper_bounds)\n",
    "\n",
    "    def is_parameter_defined(self, parameter_name: str) -> bool:\n",
    "        is_defined = False\n",
    "        objective_and_constraints = (\n",
    "            self._constraints + [self._objective]\n",
    "            if self._objective is not None\n",
    "            else self._constraints\n",
    "        )\n",
    "        for expr in objective_and_constraints:\n",
    "            params = [\n",
    "                arg for arg in _get_all_args(expr) if isinstance(arg, cp.Parameter)\n",
    "            ]\n",
    "            for param in params:\n",
    "                if param.name() == parameter_name and not is_defined:\n",
    "                    is_defined = True\n",
    "                elif param.name() == parameter_name and is_defined:\n",
    "                    raise exceptions.InstantiationError(\n",
    "                        \"Parameter name defined multiple times\"\n",
    "                    )\n",
    "        return is_defined\n",
    "\n",
    "    def update_parameter_value(self, parameter_name: str, new_value: float) -> None:\n",
    "        if not self.is_parameter_defined(parameter_name):\n",
    "            raise exceptions.InstantiationError(\"Parameter has not been defined\")\n",
    "        was_updated = False\n",
    "        objective_and_constraints = (\n",
    "            self._constraints + [self._objective]\n",
    "            if self._objective is not None\n",
    "            else self._constraints\n",
    "        )\n",
    "        for expr in objective_and_constraints:\n",
    "            params = [\n",
    "                arg for arg in _get_all_args(expr) if isinstance(arg, cp.Parameter)\n",
    "            ]\n",
    "            for param in params:\n",
    "                if param.name() == parameter_name:\n",
    "                    param.value = new_value\n",
    "                    was_updated = True\n",
    "        if not was_updated:\n",
    "            raise exceptions.InstantiationError(\"Parameter was not updated\")\n",
    "\n",
    "    def _solve_cvxpy_opt_problem(self):\n",
    "\n",
    "        try:\n",
    "            if self._opt is None:\n",
    "                self._opt = cp.Problem(cp.Minimize(self._objective), self._constraints)\n",
    "                self._initial_objective = self._objective.id\n",
    "                self._initial_constraint_ids = {const.id for const in self._constraints}\n",
    "            else:\n",
    "                if not self._objective.id == self._initial_objective:\n",
    "                    raise exceptions.InstantiationError(\n",
    "                        \"The objective function was changed after the initial optimization. \"\n",
    "                        \"Please create a new instance instead.\"\n",
    "                    )\n",
    "\n",
    "                constr_ids = {const.id for const in self._constraints}\n",
    "                if not constr_ids == self._initial_constraint_ids:\n",
    "                    raise exceptions.InstantiationError(\n",
    "                        \"The constraints were changed after the initial optimization. \"\n",
    "                        \"Please create a new instance instead.\"\n",
    "                    )\n",
    "            self._opt.solve(\n",
    "                solver=self._solver, verbose=self._verbose, **self._solver_options\n",
    "            )\n",
    "\n",
    "        except (TypeError, cp.DCPError) as e:\n",
    "            raise OptimizationError from e\n",
    "\n",
    "        if self._opt.status not in {\"optimal\", \"optimal_inaccurate\"}:\n",
    "            raise exceptions.OptimizationError(\n",
    "                \"Solver status: {}\".format(self._opt.status)\n",
    "            )\n",
    "        self.weights = self._w.value.round(16) + 0.0  # +0.0 removes signed zero\n",
    "        return self._make_output_weights()\n",
    "    \n",
    "    def add_objective(self, new_objective, **kwargs):\n",
    "        if self._opt is not None:\n",
    "            raise exceptions.InstantiationError(\n",
    "                \"Adding objectives to an already solved problem might have unintended consequences. \"\n",
    "                \"A new instance should be created for the new set of objectives.\"\n",
    "            )\n",
    "        self._additional_objectives.append(new_objective(self._w, **kwargs))\n",
    "\n",
    "    def add_constraint(self, new_constraint):\n",
    "        if not callable(new_constraint):\n",
    "            raise TypeError(\n",
    "                \"New constraint must be provided as a callable (e.g lambda function)\"\n",
    "            )\n",
    "        if self._opt is not None:\n",
    "            raise exceptions.InstantiationError(\n",
    "                \"Adding constraints to an already solved problem might have unintended consequences. \"\n",
    "                \"A new instance should be created for the new set of constraints.\"\n",
    "            )\n",
    "        self._constraints.append(new_constraint(self._w))\n",
    "\n",
    "    def convex_objective(self, custom_objective, weights_sum_to_one=True, **kwargs):\n",
    "        # custom_objective must have the right signature (w, **kwargs)\n",
    "        self._objective = custom_objective(self._w, **kwargs)\n",
    "\n",
    "        for obj in self._additional_objectives:\n",
    "            self._objective += obj\n",
    "\n",
    "        if weights_sum_to_one:\n",
    "            self.add_constraint(lambda w: cp.sum(w) == 1)\n",
    "\n",
    "        return self._solve_cvxpy_opt_problem()\n",
    "    \n",
    "    def nonconvex_objective(\n",
    "        self,\n",
    "        custom_objective,\n",
    "        objective_args=None,\n",
    "        weights_sum_to_one=True,\n",
    "        constraints=None,\n",
    "        solver=\"SLSQP\",\n",
    "        initial_guess=None,\n",
    "    ):\n",
    "        # Sanitise inputs\n",
    "        if not isinstance(objective_args, tuple):\n",
    "            objective_args = (objective_args,)\n",
    "\n",
    "        # Make scipy bounds\n",
    "        bound_array = np.vstack((self._lower_bounds, self._upper_bounds)).T\n",
    "        bounds = list(map(tuple, bound_array))\n",
    "\n",
    "        if initial_guess is None:\n",
    "            initial_guess = np.array([1 / self.n_assets] * self.n_assets)\n",
    "\n",
    "        # Construct constraints\n",
    "        final_constraints = []\n",
    "        if weights_sum_to_one:\n",
    "            final_constraints.append({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1})\n",
    "        if constraints is not None:\n",
    "            final_constraints += constraints\n",
    "\n",
    "        result = sco.minimize(\n",
    "            custom_objective,\n",
    "            x0=initial_guess,\n",
    "            args=objective_args,\n",
    "            method=solver,\n",
    "            bounds=bounds,\n",
    "            constraints=final_constraints,\n",
    "        )\n",
    "        self.weights = result[\"x\"]\n",
    "        return self._make_output_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca76077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    def __init__(self, optimizers: List[Tuple[str, Callable]], universe: Tree):\n",
    "        self.optimizers = optimizers\n",
    "        self.universe = universe\n",
    "        self.covariance = None\n",
    "\n",
    "    def run(self) -> Dict[str, float]:\n",
    "        allocations = {}\n",
    "        root_node = self.universe.root\n",
    "        self._optimize_node(root_node, 1, allocations)\n",
    "        return allocations\n",
    "\n",
    "    def _optimize_node(self, node: Node, depth: int, allocations: Dict[str, float], parent_weight: float = 1.0) -> None:\n",
    "        if depth == 1 and node.children:\n",
    "            optimizer_name, optimizer_func = self.optimizers[0]\n",
    "            node_weights = optimizer_func(node.children)\n",
    "            for child_node, weight in zip(node.children, node_weights):\n",
    "                allocations[child_node.name] = weight\n",
    "                self._optimize_node(child_node, depth + 1, allocations, weight)\n",
    "        elif node.children:\n",
    "            optimizer_name, optimizer_func = self.optimizers[depth - 1]\n",
    "            node_weights = optimizer_func(node.children)\n",
    "            for child_node, weight in zip(node.children, node_weights):\n",
    "                allocations[child_node.name] = weight * parent_weight\n",
    "                self._optimize_node(child_node, depth + 1, allocations, weight * parent_weight)\n",
    "        return\n",
    "\n",
    "    def _update_node(self, assumption_dict:Dict, dates:str):\n",
    "        nodes_list = self.universe.get_all_nodes()\n",
    "        for key, df in assumption_dict.items():\n",
    "            series_at_time = df.loc[dates,:]\n",
    "            for node in nodes_list:\n",
    "                node.params[key] = series_at_time[node.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf93d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_parity_optimizer(nodes: List[Node], covariance_matrix: np.ndarray = None) -> List[float]:\n",
    "    \n",
    "    def risk_parity_objective(w, covariance_matrix, eps=1e-8):\n",
    "        portfolio_variance = w.T @ covariance_matrix @ w\n",
    "        asset_contributions = w * np.diag(covariance_matrix @ w)\n",
    "        log_asset_contributions = np.log(asset_contributions + eps)\n",
    "        return np.sum((log_asset_contributions - log_asset_contributions.mean())**2)\n",
    "    \n",
    "    if covariance_matrix is None:\n",
    "        covariance_matrix = random_covariance_matrix(nodes)\n",
    "\n",
    "    n_assets = covariance_matrix.shape[0]\n",
    "\n",
    "    # Create the optimizer\n",
    "    opt = BaseConvexOptimizer(\n",
    "        n_assets=n_assets,\n",
    "        tickers=[node.name for node in nodes],\n",
    "        weight_bounds=(0, 1),\n",
    "    )\n",
    "\n",
    "    # Use nonconvex_objective method with the risk_parity_objective function\n",
    "    opt.nonconvex_objective(\n",
    "        risk_parity_objective,\n",
    "        objective_args=(covariance_matrix,),\n",
    "        weights_sum_to_one=True,\n",
    "    )\n",
    "\n",
    "    return list(opt.clean_weights().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c199c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def black_litterman_optimizer(\n",
    "    nodes: List[Node],\n",
    "    covariance_matrix: Optional[np.ndarray] = None,\n",
    "    tau: float = 0.05,\n",
    "    P: Optional[np.ndarray] = None,\n",
    "    Q: Optional[np.ndarray] = None,\n",
    "    omega: Optional[np.ndarray] = None,\n",
    "    delta: float = 1,\n",
    ") -> List[float]:\n",
    "\n",
    "    def black_litterman_objective(w, mu, covariance_matrix, tau, P, Q, omega, delta):\n",
    "        w_eq = np.linalg.inv(delta * covariance_matrix) @ mu\n",
    "        pi = delta * covariance_matrix @ w_eq\n",
    "        sigma = covariance_matrix + tau * P.T @ np.linalg.inv(omega) @ P\n",
    "        mu_bl = np.linalg.inv(np.linalg.inv(tau * covariance_matrix) + P.T @ np.linalg.inv(omega) @ P) @ (np.linalg.inv(tau * covariance_matrix) @ pi + P.T @ np.linalg.inv(omega) @ Q)\n",
    "        return (w - mu_bl).T @ sigma @ (w - mu_bl)\n",
    "    \n",
    "    n_assets = len(nodes)\n",
    "    tickers = [node.name for node in nodes]\n",
    "\n",
    "    expected_returns = np.array([node.params['returns'] for node in nodes])\n",
    "\n",
    "    # Create the optimizer\n",
    "    opt = BaseConvexOptimizer(n_assets=n_assets, tickers=tickers, weight_bounds=(0, 1))\n",
    "\n",
    "    # Generate random covariance matrix if not provided\n",
    "    if covariance_matrix is None:\n",
    "        covariance_matrix = random_covariance_matrix(nodes)\n",
    "\n",
    "    # Set up views and associated matrices if not provided\n",
    "    if P is None or Q is None:\n",
    "        # Default: neutral view, no additional views\n",
    "        P = np.eye(n_assets)\n",
    "        Q = expected_returns\n",
    "\n",
    "    if omega is None:\n",
    "        omega = P @ covariance_matrix @ P.T * tau\n",
    "\n",
    "    opt.nonconvex_objective(\n",
    "        black_litterman_objective,\n",
    "        objective_args=(expected_returns, covariance_matrix, tau, P, Q, omega, delta),\n",
    "        weights_sum_to_one=True,\n",
    "    )\n",
    "\n",
    "    return list(opt.clean_weights().values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae7f695",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7120\\1689215924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SAA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"TAA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_variance_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniverse_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mallocations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# Print the allocation results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7120\\20594479.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mallocations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mroot_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniverse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimize_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallocations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mallocations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7120\\20594479.py\u001b[0m in \u001b[0;36m_optimize_node\u001b[1;34m(self, node, depth, allocations, parent_weight)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0moptimizer_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mnode_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mchild_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mallocations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7120\\1689215924.py\u001b[0m in \u001b[0;36mtest_optimizer\u001b[1;34m(nodes)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseConvexOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_assets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtickers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonconvex_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mmean_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_returns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_sum_to_one\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7120\\181091061.py\u001b[0m in \u001b[0;36mnonconvex_objective\u001b[1;34m(self, custom_objective, objective_args, weights_sum_to_one, constraints, solver, initial_guess)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mfinal_constraints\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         result = sco.minimize(\n\u001b[0m\u001b[0;32m    239\u001b[0m             \u001b[0mcustom_objective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_guess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    706\u001b[0m                                 **options)\n\u001b[0;32m    707\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'slsqp'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m         res = _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[0;32m    709\u001b[0m                               constraints, callback=callback, **options)\n\u001b[0;32m    710\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'trust-constr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_slsqp_py.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;31m# ScalarFunction provides function and gradient evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m     sf = _prepare_scalar_function(func, x, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[0;32m    375\u001b[0m                                   \u001b[0mfinite_diff_rel_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinite_diff_rel_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                                   bounds=new_bounds)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[0;32m    264\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;31m# Gradient evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;31m# Make sure the function returns a true scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "def test_optimizer(nodes: List[Node]) -> List[float]:\n",
    "    \n",
    "    def mean_return(weights, expected_returns):\n",
    "        return weights @ expected_returns\n",
    "\n",
    "    n_assets = len(nodes)\n",
    "    tickers = [node.name for node in nodes]\n",
    "\n",
    "    expected_returns = np.array([node.params['returns'] for node in nodes])\n",
    "\n",
    "    optimizer = BaseConvexOptimizer(n_assets, tickers=tickers)\n",
    "\n",
    "    result = optimizer.nonconvex_objective(lambda w: -mean_return(w, expected_returns), weights_sum_to_one=True)\n",
    "\n",
    "    weights = list(result.values())\n",
    "    return weights\n",
    "\n",
    "\n",
    "def equal_weight_optimizer(nodes: List[Node]) -> List[float]:\n",
    "    n = len(nodes)\n",
    "    return [1.0 / n] * n\n",
    "\n",
    "# covariance_matrix 없어서 안됨. 우선 ticker을 기준으로 데이터끌어와서 covariacne_matrix, expected_return, variance return 만들어야 할듯\n",
    "def mean_variance_optimizer(nodes: List[Node], covariance_matrix: Optional[np.ndarray] = None, risk_aversion: float = 1.0) -> List[float]:\n",
    "    n_assets = len(nodes)\n",
    "    tickers = [node.name for node in nodes]\n",
    "\n",
    "    expected_returns = np.array([node.params['returns'] for node in nodes])\n",
    "    \n",
    "    optimizer = BaseConvexOptimizer(n_assets, tickers=tickers)\n",
    "    covariance_matrix = get_covariance_matrix(nodes)\n",
    "    \n",
    "    if isinstance(covariance_matrix, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        covariance_matrix = random_covariance_matrix(nodes)\n",
    "    \n",
    "    #랜덤 공분산 행렬 생성 테스트\n",
    "    #if covariance_matrix == None:\n",
    "    #    covariance_matrix = random_covariance_matrix(nodes)\n",
    "    \n",
    "    optimizer.convex_objective(\n",
    "        lambda w: risk_aversion * portfolio_variance(w, covariance_matrix) - mean_return(w, expected_returns),\n",
    "        weights_sum_to_one=True\n",
    "    )\n",
    "\n",
    "    weights = list(optimizer.clean_weights().values())\n",
    "    return weights\n",
    "\n",
    "def portfolio_variance(weights, covariance_matrix):\n",
    "    return cp.quad_form(weights, covariance_matrix)\n",
    "\n",
    "def mean_return(weights, expected_returns):\n",
    "    return weights @ expected_returns\n",
    "\n",
    "# 돌아가는지 확인을 위한 랜덤 공분산 행렬 함수\n",
    "def random_covariance_matrix(nodes: List[Node]):\n",
    "    size = len(nodes)\n",
    "    rnd = np.random.rand(size, size)\n",
    "    return np.matmul(rnd, rnd.T)\n",
    "\n",
    "\n",
    "# Create a tree and add nodes\n",
    "universe_1 = Tree(\"Universe\")\n",
    "universe_1.insert(\"Universe\", \"Stocks\", returns = 0.1, variance = 0.15)\n",
    "universe_1.insert(\"Universe\", \"Bonds\", returns = 0.05, variance = 0.1)\n",
    "universe_1.insert(\"Stocks\", \"Korean Stocks\", returns = 0.12, variance = 0.18)\n",
    "universe_1.insert(\"Stocks\", \"US Stocks\", returns = 0.08, variance = 0.14)\n",
    "universe_1.insert(\"Bonds\", \"Government Bonds\", returns = 0.04, variance = 0.09)\n",
    "universe_1.insert(\"Bonds\", \"Corporate Bonds\", returns = 0.06, variance = 0.12)\n",
    "\n",
    "pipe = pipeline([(\"SAA\", test_optimizer), (\"TAA\", mean_variance_optimizer)], universe_1)\n",
    "\n",
    "allocations = pipe.run()\n",
    "\n",
    "# Print the allocation results\n",
    "for asset, weight in allocations.items():\n",
    "    print(f\"{asset}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline([(\"SAA\", risk_parity_optimizer), (\"TAA\", black_litterman_optimizer)], universe_1)\n",
    "\n",
    "allocations = pipe.run()\n",
    "\n",
    "# Print the allocation results\n",
    "for asset, weight in allocations.items():\n",
    "    print(f\"{asset}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline([(\"SAA\", risk_parity_optimizer), (\"TAA\", mean_variance_optimizer)], universe_1)\n",
    "\n",
    "allocations = pipe.run()\n",
    "\n",
    "# Print the allocation results\n",
    "for asset, weight in allocations.items():\n",
    "    print(f\"{asset}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_1.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f07dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Backtest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline: pipeline,\n",
    "        loader: Loader,\n",
    "        assumption: AssetAssumption,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        rebalancing_frequency: str = \"1m\",\n",
    "        rebalancing_fee: float = 0.001,\n",
    "    ):\n",
    "        self.pipeline = pipeline\n",
    "        self.loader = loader\n",
    "        self.start_date = pd.to_datetime(start_date)\n",
    "        self.end_date = pd.to_datetime(end_date)\n",
    "        self.rebalancing_frequency = rebalancing_frequency\n",
    "        self.rebalancing_fee = rebalancing_fee\n",
    "        self.assumption = assumption\n",
    "    \n",
    "        \n",
    "    def previous_business_day(self, date: datetime) -> datetime:\n",
    "        date_series = pd.date_range(end=date, periods=2, freq='B')\n",
    "        return date_series[0]\n",
    "        \n",
    "    def update_expected_returns(self, prices: pd.DataFrame, nodes: List[Node]):\n",
    "        # Implement the logic to update expected returns for each node based on the given prices\n",
    "        pass\n",
    "\n",
    "    def _generate_rebalancing_dates(self, date_index):\n",
    "        rebalancing_dates = pd.date_range(self.start_date, self.end_date, freq=self.rebalancing_frequency)\n",
    "        return rebalancing_dates.intersection(date_index)\n",
    "\n",
    "    def run_backtest(self):\n",
    "        prices = self.loader.load_data(self.start_date, self.end_date)\n",
    "        assumption_dict = self.assumption.get_data(self.loader, self.start_date, self.end_date)\n",
    "        \n",
    "        # def _update_node(self, assumption_dict:Dict, dates:str)\n",
    "        self.pipeline._update_node(assumption_dict, self.start_date)\n",
    "        \n",
    "        rebalancing_dates = self._generate_rebalancing_dates(prices.index)\n",
    "\n",
    "        # Include the start date as a rebalancing date\n",
    "        rebalancing_dates = rebalancing_dates.insert(0, self.start_date)\n",
    "\n",
    "        asset_weights = []\n",
    "        portfolio_value = [1]\n",
    "\n",
    "        for i, rebalancing_date in enumerate(rebalancing_dates):\n",
    "            prices_sub = prices.loc[:rebalancing_date]\n",
    "            \n",
    "            \"\"\"\n",
    "            노드의 ASSUMPTION 파라미터 업데이트가 일어나는 곳\n",
    "            \"\"\"\n",
    "            \n",
    "            allocations = self.pipeline.run()\n",
    "\n",
    "            asset_weights.append(allocations)\n",
    "\n",
    "            if i < len(rebalancing_dates) - 1:\n",
    "                end_date = rebalancing_dates[i + 1]\n",
    "            else:\n",
    "                end_date = prices.index[-1]\n",
    "\n",
    "            start_date = self.previous_business_day(rebalancing_date)\n",
    "            prices_period = prices.loc[start_date:end_date]\n",
    "            period_returns = prices_period.pct_change().dropna()\n",
    "            period_returns['portfolio'] = np.dot(period_returns, list(allocations.values()))\n",
    "\n",
    "            # Apply rebalancing_fee\n",
    "            if i > 0:\n",
    "                period_returns.loc[rebalancing_date, 'portfolio'] -= self.rebalancing_fee\n",
    "\n",
    "            period_cumulative_returns = (1 + period_returns).cumprod()\n",
    "            period_portfolio_value = portfolio_value[-1] * period_cumulative_returns['portfolio']\n",
    "            \n",
    "            if i > 0:\n",
    "                portfolio_value.extend(period_portfolio_value[1:].values)\n",
    "            else:\n",
    "                portfolio_value.extend(period_portfolio_value[:].values)\n",
    "\n",
    "        self.portfolio_value = pd.Series(portfolio_value, index=prices.index)\n",
    "        self.returns = self.portfolio_value.pct_change().dropna()\n",
    "        self.asset_weights = pd.DataFrame(asset_weights, index=rebalancing_dates, columns=prices.columns)\n",
    "\n",
    "    def plot_performance(self):\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(self.portfolio_value)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.title(\"Portfolio Performance\")\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_maximum_drawdown(self):\n",
    "        rolling_max = self.portfolio_value.cummax()\n",
    "        drawdowns = (self.portfolio_value - rolling_max) / rolling_max\n",
    "        return drawdowns.min()\n",
    "\n",
    "    def calculate_turnover(self):\n",
    "        turnover = (self.asset_weights.shift(1) - self.asset_weights).abs().sum(axis=1).mean()\n",
    "        return turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load asset prices\n",
    "loader = Loader(\"./sample.csv\")\n",
    "\n",
    "# Create a Pipeline object\n",
    "#pipeline = Pipeline(...)\n",
    "\n",
    "assumption = AssetAssumption(returns=(historical_return, {'window': 20}), \n",
    "                             covariance=(historical_covariance, {'window': 60}))\n",
    "\n",
    "# Create a Backtest object\n",
    "backtest = Backtest(\n",
    "    pipeline=pipe,\n",
    "    loader=loader,\n",
    "    assumption=assumption,\n",
    "    start_date=\"2022-01-03\",\n",
    "    end_date=\"2023-05-01\",\n",
    ")\n",
    "\n",
    "\n",
    "# Run the backtest\n",
    "backtest.run_backtest()\n",
    "\n",
    "# Plot the performance\n",
    "backtest.plot_performance()\n",
    "\n",
    "# Calculate maximum drawdown and turnover\n",
    "max_drawdown = backtest.calculate_maximum_drawdown()\n",
    "turnover = backtest.calculate_turnover()\n",
    "\n",
    "print(\"Max Drawdown:\", max_drawdown)\n",
    "print(\"Turnover:\", turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loader.load_data('2021-07-21', '2023-04-28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pct_change().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c4250",
   "metadata": {},
   "source": [
    "# 잠시 샘플 데이터 크롤링해서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfda9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import FinanceDataReader as fdr\n",
    "# 회사채 : 136340, 국공채 : 272560, 미국주식 : 379780, 한국주식 : 148020\n",
    "# 주식 : 189400 채권 : 267440\n",
    "ticker=['189400', '267440', '379780', '148020', '136340', '272580']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(tickers):\n",
    "    df = dict()\n",
    "    name = ['Stocks', 'Bonds', 'US Stocks', 'Korean Stocks', 'Government Bonds', 'Corporate Bonds']\n",
    "    for i in range(len(name)):\n",
    "        df[name[i]] = fdr.DataReader(tickers[i], '2018')['Close']\n",
    "        \n",
    "    return pd.DataFrame(df).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 백업용\n",
    "\"\"\"\n",
    "    def run_backtest(self):\n",
    "        prices = self.loader.load_data(self.start_date, self.end_date)\n",
    "        rebalancing_dates = self._generate_rebalancing_dates(prices.index)\n",
    "\n",
    "        asset_weights = []\n",
    "        portfolio_value = [1]\n",
    "        \n",
    "        for i, rebalancing_date in enumerate(rebalancing_dates[:-1]):\n",
    "            prices_sub = prices.loc[:rebalancing_date]\n",
    "            allocations = self.pipeline.run()\n",
    "\n",
    "            asset_weights.append(allocations)\n",
    "            start_date = self.previous_business_day(rebalancing_date)\n",
    "            end_date = rebalancing_dates[i + 1]\n",
    "\n",
    "            prices_period = prices.loc[start_date:end_date]\n",
    "            period_returns = prices_period.pct_change().dropna()\n",
    "            #print(period_returns)\n",
    "            period_returns['portfolio'] = np.dot(period_returns, list(allocations.values()))\n",
    "\n",
    "            # Apply rebalancing_fee\n",
    "            if i > 0:\n",
    "                period_returns.loc[rebalancing_date, 'portfolio'] -= self.rebalancing_fee\n",
    "\n",
    "            period_cumulative_returns = (1 + period_returns).cumprod()\n",
    "            period_portfolio_value = portfolio_value[-1] * period_cumulative_returns['portfolio']\n",
    "            print(period_portfolio_value[1:])\n",
    "            portfolio_value.extend(period_portfolio_value[1:].values)\n",
    "\n",
    "        self.portfolio_value = pd.Series(portfolio_value, index=prices.index)\n",
    "        self.returns = self.portfolio_value.pct_change().dropna()\n",
    "        self.asset_weights = pd.DataFrame(asset_weights, index=rebalancing_dates[:-1], columns=prices.columns)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sample = get_ticker_data(ticker)\n",
    "sample.to_csv('sample.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94564415",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1, 'b':2}\n",
    "\n",
    "for key, value in a.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2021-07-22', 'Stocks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
